{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense,GlobalAveragePooling2D,MaxPooling2D,Activation,Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam    \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# freeze the layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a custom classifier\n",
    "flat = Flatten()(base_model.output)\n",
    "dens= Dense(128, activation='relu')(flat)  # we can add more layers here\n",
    "output = Dense(1, activation='softmax')(dens)\n",
    "model = Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160 images belonging to 2 classes.\n",
      "Found 24 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing    \n",
    "img_width, img_height = 224, 224\n",
    "# some prameters\n",
    "train_data_dir = 'DS'\n",
    "validation_data_dir = 'val'\n",
    "batch_size = 32\n",
    "# some img transforamtiona\n",
    "# some img transforamtiona\n",
    "train_datagen=ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True,height_shift_range=0.2,width_shift_range=0.2,rotation_range=20)\n",
    "validation_datagen=ImageDataGenerator(rescale=1./255)\n",
    "np.random.seed(42)\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(train_data_dir,target_size=(img_height,img_width),batch_size=batch_size,class_mode='binary')# set as training data\n",
    "validition_generator=validation_datagen.flow_from_directory(validation_data_dir,target_size=(img_height,img_width),batch_size=batch_size,class_mode='binary')# set as validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/quant/lib/python3.12/site-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/quant/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "2024-09-09 18:26:54.699254: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n",
      "2024-09-09 18:26:55.066912: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n",
      "2024-09-09 18:26:55.466194: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n",
      "2024-09-09 18:26:55.782594: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n",
      "2024-09-09 18:26:56.137376: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 21s/step - accuracy: 0.5013 - loss: 2.3735 - val_accuracy: 0.5000 - val_loss: 1.2003\n",
      "Epoch 2/10\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 14s/step - accuracy: 0.4766 - loss: 1.0690"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 18:28:24.135317: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/opt/conda/envs/quant/lib/python3.12/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 13s/step - accuracy: 0.4792 - loss: 1.0172 - val_accuracy: 0.5000 - val_loss: 0.9907\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 20s/step - accuracy: 0.5104 - loss: 0.7513 - val_accuracy: 0.5000 - val_loss: 0.4210\n",
      "Epoch 4/10\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 11s/step - accuracy: 0.4531 - loss: 0.4562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 18:29:55.159206: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10s/step - accuracy: 0.4583 - loss: 0.4472 - val_accuracy: 0.5000 - val_loss: 0.4297\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 16s/step - accuracy: 0.5117 - loss: 0.3955 - val_accuracy: 0.5000 - val_loss: 0.2480\n",
      "Epoch 6/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - accuracy: 0.5104 - loss: 0.4335 - val_accuracy: 0.5000 - val_loss: 0.2457\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 16s/step - accuracy: 0.4128 - loss: 0.3322 - val_accuracy: 0.5000 - val_loss: 0.2217\n",
      "Epoch 8/10\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 11s/step - accuracy: 0.6172 - loss: 0.3680"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 18:32:25.905677: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - accuracy: 0.6146 - loss: 0.3607 - val_accuracy: 0.5000 - val_loss: 0.2144\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 16s/step - accuracy: 0.5091 - loss: 0.3083 - val_accuracy: 0.5000 - val_loss: 0.1673\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - accuracy: 0.4792 - loss: 0.2983 - val_accuracy: 0.5000 - val_loss: 0.1804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x73504d368c20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model   \n",
    "model.fit(train_generator, steps_per_epoch=100 // batch_size, epochs=10, validation_data=validition_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.5000 - loss: 0.1804\n",
      "Accuracy:  [0.18037836253643036, 0.5]\n"
     ]
    }
   ],
   "source": [
    "# accurcey print\n",
    "acc = model.evaluate(validition_generator)     \n",
    "print('Accuracy: ', acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfreeze the layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 43s/step - accuracy: 0.4792 - loss: 15.2480 - val_accuracy: 0.5000 - val_loss: 0.9306\n",
      "Epoch 2/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 23s/step - accuracy: 0.5208 - loss: 1.8284 - val_accuracy: 0.5000 - val_loss: 0.6873\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 42s/step - accuracy: 0.4935 - loss: 0.8070 - val_accuracy: 0.5000 - val_loss: 0.7987\n",
      "Epoch 4/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 23s/step - accuracy: 0.4792 - loss: 0.8692 - val_accuracy: 0.5000 - val_loss: 1.0609\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 42s/step - accuracy: 0.5560 - loss: 0.7507 - val_accuracy: 0.5000 - val_loss: 0.6909\n",
      "Epoch 6/10\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 38s/step - accuracy: 0.5000 - loss: 0.7352 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 18:44:29.690606: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 23s/step - accuracy: 0.4792 - loss: 0.7507 - val_accuracy: 0.5000 - val_loss: 0.6882\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 43s/step - accuracy: 0.5990 - loss: 0.6860 - val_accuracy: 0.5000 - val_loss: 0.6871\n",
      "Epoch 8/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 23s/step - accuracy: 0.4271 - loss: 0.7021 - val_accuracy: 0.5000 - val_loss: 0.6909\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 42s/step - accuracy: 0.5495 - loss: 0.7100 - val_accuracy: 0.5000 - val_loss: 0.7791\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 23s/step - accuracy: 0.4688 - loss: 0.7485 - val_accuracy: 0.5000 - val_loss: 0.6876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x73504d1e7530>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(train_generator, steps_per_epoch=100 // batch_size, epochs=10, validation_data=validition_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.5000 - loss: 0.6876\n",
      "[0.6875918507575989, 0.5]\n"
     ]
    }
   ],
   "source": [
    "# accurcey print\n",
    "acc = model.evaluate(validition_generator)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "img_path = 'tests/d1.jpg'\n",
    "img_height, img_width = 224, 224  # Set these to match your model's input size\n",
    "img = image.load_img(img_path, target_size=(img_height, img_width))\n",
    "\n",
    "# Convert the image to a numpy array and rescale it\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "img_array /= 255.0  # Rescale to [0, 1]\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(img_array)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
